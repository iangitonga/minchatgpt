{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AAOhg04WUhY5"
   },
   "source": [
    "**GPT2 FINETUNING**\n",
    "\n",
    "The goal of the code below is to finetune the GPT2 model on Stanford Alpaca dataset to enable the model to follow instructions prompted by humans similar to ChatGPT or Alpaca models.Stanford Alpaca dataset contains data with instructions, input and output. For simplicity, the data with input is not used in finetuning because we will not be using inputs during inference, atleast for now. We finetune the model on a single GPU with 32-bit precision using Adam optimizer with a cosine schedule for the learning rate.\n",
    "\n",
    "In the future, the code will be improved to allow finetuning of the GPT2-xl model with over 1B parameters. Possible techniques to allow finetuning large models include using FP16, allow multi-gpu setup and using LORA or QLORA finetuning methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "id": "y7nB58V4QFm_"
   },
   "outputs": [],
   "source": [
    "# Fetch the model.\n",
    "# Model can be medium(12GB vram) or large(32GB vram or 20GB if FP16 is enabled).\n",
    "MODEL = \"medium\"\n",
    "\n",
    "if MODEL == \"medium\":\n",
    "    !wget https://huggingface.co/gpt2-medium/resolve/main/pytorch_model.bin\n",
    "else:\n",
    "    !wget https://huggingface.co/gpt2-large/resolve/main/pytorch_model.bin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "a0U90gk8QFnD"
   },
   "outputs": [],
   "source": [
    "# Fetch the data.\n",
    "!wget https://github.com/tatsu-lab/stanford_alpaca/raw/main/alpaca_data.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T10:45:07.912771Z",
     "iopub.status.busy": "2023-09-10T10:45:07.912408Z",
     "iopub.status.idle": "2023-09-10T10:45:21.350161Z",
     "shell.execute_reply": "2023-09-10T10:45:21.349002Z",
     "shell.execute_reply.started": "2023-09-10T10:45:07.912739Z"
    },
    "id": "-bBYt4cNQFnE"
   },
   "outputs": [],
   "source": [
    "# Install dependencies.\n",
    "!pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "9aamoWqbQFnF"
   },
   "outputs": [],
   "source": [
    "# HYPERPARAMETERS AND OPTIONS\n",
    "\n",
    "eval_iters = 100\n",
    "num_epochs = 2\n",
    "# To simulate batch size.\n",
    "grad_accum_steps = 8\n",
    "learning_rate = 6.5e-5\n",
    "min_learning_rate = learning_rate / 10  # As per Chinchilla paper.\n",
    "warmup_iters = int(0.2 * 2000) # 2pc of training warmup steps as per GPT1 paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T10:45:21.352826Z",
     "iopub.status.busy": "2023-09-10T10:45:21.352320Z",
     "iopub.status.idle": "2023-09-10T10:45:24.124317Z",
     "shell.execute_reply": "2023-09-10T10:45:24.123357Z",
     "shell.execute_reply.started": "2023-09-10T10:45:21.352791Z"
    },
    "id": "5rmbwLb7QFnF"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import tiktoken\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wYqmAgJ3Sc6Z"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"DEVICE: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T11:59:12.270412Z",
     "iopub.status.busy": "2023-09-10T11:59:12.269865Z",
     "iopub.status.idle": "2023-09-10T11:59:12.315248Z",
     "shell.execute_reply": "2023-09-10T11:59:12.313889Z",
     "shell.execute_reply.started": "2023-09-10T11:59:12.270382Z"
    },
    "id": "Vlkhv5gHQFnG"
   },
   "outputs": [],
   "source": [
    "# MODEL DEFINITION\n",
    "\n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    n_vocab: int =  50257\n",
    "    n_ctx: int = 1024\n",
    "    n_state: int = 0\n",
    "    n_layer: int = 0\n",
    "    n_head: int = 0\n",
    "    attn_pdrop: float = 0.1\n",
    "    resid_pdrop: float = 0.1\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.n_head = config.n_head\n",
    "        self.n_state = config.n_state\n",
    "        self.c_attn = nn.Linear(config.n_state, config.n_state * 3)\n",
    "        self.c_proj = nn.Linear(config.n_state, config.n_state)\n",
    "        self.attn_pdrop = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_pdrop = nn.Dropout(config.resid_pdrop)\n",
    "\n",
    "        # The masking attn mask.\n",
    "        bias = torch.tril(torch.ones(config.n_ctx, config.n_ctx)).view(1, 1, config.n_ctx, config.n_ctx)\n",
    "        self.register_buffer('bias', bias, persistent=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Computes self-attention between `x` and itself.\n",
    "\n",
    "        Args:\n",
    "            x: A tensor of shape (batch_size, n_ctx, n_state).\n",
    "\n",
    "        Returns:\n",
    "            A tensor of shape (batch_size, n_ctx, n_state).\n",
    "        \"\"\"\n",
    "        q, k, v = self.c_attn(x).split(self.n_state, dim=2)\n",
    "        qkv = self._qkv_attention(q, k, v)\n",
    "        out = self.resid_pdrop(self.c_proj(qkv))\n",
    "        return out\n",
    "\n",
    "    def _qkv_attention(self, q, k, v):\n",
    "        n_batch, n_ctx = q.shape[0], q.shape[1]\n",
    "        d_head = self.n_state // self.n_head\n",
    "        q = q.view(n_batch, n_ctx, self.n_head, d_head).permute(0, 2, 1, 3)\n",
    "        k = k.view(n_batch, n_ctx, self.n_head, d_head).permute(0, 2, 3, 1)\n",
    "        v = v.view(n_batch, n_ctx, self.n_head, d_head).permute(0, 2, 1, 3)\n",
    "        scale = 1.0 / math.sqrt(d_head)\n",
    "        qk = (q @ k) * scale\n",
    "        qk = qk.masked_fill(self.bias[:, :, :n_ctx, :n_ctx] == 0, float('-inf'))\n",
    "        qk = F.softmax(qk, dim=-1)\n",
    "        qk = self.attn_pdrop(qk)\n",
    "        qkv = qk @ v\n",
    "        qkv = qkv.permute(0, 2, 1, 3).flatten(start_dim=2)\n",
    "        return qkv\n",
    "\n",
    "\n",
    "class ResidualAttentionBlock(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.attn = MultiHeadSelfAttention(config)\n",
    "        self.ln_1 = nn.LayerNorm(config.n_state)\n",
    "        self.mlp = nn.ModuleDict(dict(\n",
    "            c_fc    = nn.Linear(config.n_state, config.n_state * 4),\n",
    "            c_proj  = nn.Linear(config.n_state * 4, config.n_state),\n",
    "            act     = nn.GELU(approximate=\"tanh\"),\n",
    "            dropout = nn.Dropout(config.resid_pdrop),\n",
    "        ))\n",
    "        self.mlpf = lambda x: self.mlp.dropout(self.mlp.c_proj(self.mlp.act(self.mlp.c_fc(x)))) # MLP forward\n",
    "        self.ln_2 = nn.LayerNorm(config.n_state)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlpf(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.config = config\n",
    "        self.wte = nn.Embedding(config.n_vocab, config.n_state)\n",
    "        self.wpe = nn.Embedding(config.n_ctx, config.n_state)\n",
    "        blocks = []\n",
    "        self.n_layer_half = config.n_layer//2\n",
    "        for i in range(self.n_layer_half):\n",
    "            blocks.append(ResidualAttentionBlock(config))\n",
    "        for i in range(self.n_layer_half):\n",
    "            blocks.append(ResidualAttentionBlock(config))\n",
    "        self.h = nn.ModuleList(blocks)\n",
    "        self.ln_f = nn.LayerNorm(config.n_state)\n",
    "\n",
    "        # init all weights, and apply a special scaled init to the residual projections, per GPT-2 paper\n",
    "        self.apply(self._init_weights)\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            torch.nn.init.zeros_(module.bias)\n",
    "            torch.nn.init.ones_(module.weight)\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        pos = torch.arange(0, x.shape[1], dtype=torch.long).unsqueeze(0).to(device)\n",
    "        x = self.wte(x) + self.wpe(pos)\n",
    "        for block in self.h:\n",
    "            x = block(x)\n",
    "        x = self.ln_f(x)\n",
    "        logits = (x @ torch.transpose(self.wte.weight.to(x.dtype), 0, 1)).float()\n",
    "        if y is not None:\n",
    "            loss = self.compute_loss(logits, y)\n",
    "            return loss\n",
    "        else:\n",
    "            return logits\n",
    "\n",
    "    def compute_loss(self, logits, targets):\n",
    "        loss = nn.CrossEntropyLoss()\n",
    "        batch_size, n_ctx, num_classes = logits.shape\n",
    "        logits = logits.view(batch_size * n_ctx, num_classes)\n",
    "        targets = targets.view(batch_size * n_ctx)\n",
    "        loss = loss(logits, targets)\n",
    "        return loss\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, prompt_text, tokenizer, top_k=40, temp=1.0):\n",
    "        self.eval()\n",
    "        prompt_tokens = tokenizer.encode(prompt_text)\n",
    "        tokens = torch.tensor([prompt_tokens])\n",
    "        max_ctx_size = 1024\n",
    "        n_iter = max_ctx_size - len(tokens)\n",
    "        for i in range(n_iter):\n",
    "            logits = self(tokens.to(device)).cpu()\n",
    "            logits = logits[:, -1]\n",
    "            logits = logits / temp\n",
    "            logits[:, [21017, 4242, 2235, 2]] = float(\"-inf\")\n",
    "            v, _ = torch.topk(logits, top_k)\n",
    "            logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            pred = torch.multinomial(probs, num_samples=1)\n",
    "            pred_token = pred.item()\n",
    "            if pred_token == tokenizer.eot_token:\n",
    "                    break\n",
    "            print(tokenizer.decode([pred_token]), end=\"\", flush=True)\n",
    "            tokens = torch.cat((tokens, pred), dim=1)\n",
    "        self.train()\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, path, config):\n",
    "        model = cls(config)\n",
    "        gpt_state = torch.load(path, map_location=\"cpu\")\n",
    "        for key in gpt_state.keys():\n",
    "            if (key.endswith(\"attn.c_attn.weight\")\n",
    "                or key.endswith(\"attn.c_proj.weight\")\n",
    "                or key.endswith(\"mlp.c_fc.weight\")\n",
    "                or key.endswith(\"mlp.c_proj.weight\")):\n",
    "                gpt_state[key] = gpt_state[key].transpose(0, 1)\n",
    "        model.load_state_dict(gpt_state)\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T13:36:28.364488Z",
     "iopub.status.busy": "2023-09-10T13:36:28.364091Z",
     "iopub.status.idle": "2023-09-10T13:36:41.886506Z",
     "shell.execute_reply": "2023-09-10T13:36:41.885484Z",
     "shell.execute_reply.started": "2023-09-10T13:36:28.364457Z"
    },
    "id": "r_AxyRvXQFnK"
   },
   "outputs": [],
   "source": [
    "md_model_config = ModelConfig(\n",
    "    n_state=1024,\n",
    "    n_layer=24,\n",
    "    n_head=16\n",
    ")\n",
    "\n",
    "model = Transformer.from_pretrained(\"pytorch_model.bin\", md_model_config)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T10:49:52.703306Z",
     "iopub.status.busy": "2023-09-10T10:49:52.702609Z",
     "iopub.status.idle": "2023-09-10T10:49:59.368693Z",
     "shell.execute_reply": "2023-09-10T10:49:59.367652Z",
     "shell.execute_reply.started": "2023-09-10T10:49:52.703272Z"
    },
    "id": "knSuOxDqQFnL"
   },
   "outputs": [],
   "source": [
    "def format_instruction(instruction):\n",
    "    formatted = f\"Below is an instruction that describes a task, paired with an input that\" \\\n",
    "      \" provides further context. Write a response that appropriately completes the request.\\n\" \\\n",
    "      f\"### Instruction: {instruction['instruction']}\\n### Response: {instruction['output']}\"\n",
    "    return formatted\n",
    "\n",
    "\n",
    "with open(\"alpaca_data.json\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "\n",
    "dataset = []\n",
    "max_ctx_size = 1024\n",
    "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "for d in data:\n",
    "    # Skip instructions with input.\n",
    "    if d['input'] == '':\n",
    "        continue\n",
    "    prompt = format_instruction(d)\n",
    "    tokens = tokenizer.encode(prompt) + [tokenizer.eot_token]\n",
    "    if len(tokens) <= max_ctx_size:\n",
    "        x = tokens[:-1]\n",
    "        y = tokens[1:]\n",
    "        dataset.append((x, y))\n",
    "\n",
    "random.shuffle(dataset)\n",
    "val_idx = int(0.1 * len(dataset))\n",
    "train_dataset = dataset[val_idx:]\n",
    "val_dataset = dataset[:val_idx]\n",
    "print(f\"train data size: {len(train_dataset)}\")\n",
    "print(f\"val data size: {len(val_dataset)}\")\n",
    "\n",
    "\n",
    "def fetch_sample(split):\n",
    "    dataset = train_dataset if split == 'train' else val_dataset\n",
    "    batch_size = 1\n",
    "    index = random.randint(0, len(dataset))\n",
    "    x, y = dataset[index]\n",
    "    x = torch.tensor([x], dtype=torch.long).view((1, len(x)))\n",
    "    y = torch.tensor([y], dtype=torch.long).view((1, len(y)))\n",
    "    x, y = x.to(device), y.to(device)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T10:50:12.841349Z",
     "iopub.status.busy": "2023-09-10T10:50:12.840970Z",
     "iopub.status.idle": "2023-09-10T10:50:12.848948Z",
     "shell.execute_reply": "2023-09-10T10:50:12.847857Z",
     "shell.execute_reply.started": "2023-09-10T10:50:12.841319Z"
    },
    "id": "_fRSRF2GQFnL"
   },
   "outputs": [],
   "source": [
    "# METRICS\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_loss(split='train'):\n",
    "    model.eval()\n",
    "    losses = []\n",
    "    for i in range(eval_iters):\n",
    "        x, y = fetch_sample(split=split)\n",
    "        loss = model(x, y)\n",
    "        losses.append(loss.item())\n",
    "    mean_loss = sum(losses) / len(losses)\n",
    "    model.train()\n",
    "    return mean_loss\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate_val_perplexity():\n",
    "    model.eval()\n",
    "    nlls = []\n",
    "    for x, y in val_dataset:\n",
    "        x = torch.tensor(x, dtype=torch.long).view((1, len(x)))\n",
    "        y = torch.tensor(y, dtype=torch.long).view((1, len(y)))\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        loss = model(x, y)\n",
    "        nlls.append(loss.cpu())\n",
    "    ppl = torch.exp(torch.stack(nlls).mean())\n",
    "    model.train()\n",
    "    return ppl\n",
    "\n",
    "\n",
    "train_loss = evaluate_loss(split=\"train\")\n",
    "val_loss = evaluate_loss(split=\"val\")\n",
    "print(f\"Initial train loss: {train_loss:.4f}\\nInitial val loss: {val_loss:.4f}\")\n",
    "val_ppl = evaluate_val_perplexity()\n",
    "print(f\"Initial val perplexity: {val_ppl:.4f}\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T12:34:41.895564Z",
     "iopub.status.busy": "2023-09-10T12:34:41.895178Z",
     "iopub.status.idle": "2023-09-10T12:34:41.905293Z",
     "shell.execute_reply": "2023-09-10T12:34:41.904326Z",
     "shell.execute_reply.started": "2023-09-10T12:34:41.895533Z"
    },
    "id": "L8h-GUjRQFnM"
   },
   "outputs": [],
   "source": [
    "# OPTIMIZER CONFIG\n",
    "\n",
    "n_iters = int(len(train_dataset) / grad_accum_steps * num_epochs)\n",
    "\n",
    "\n",
    "def get_lr(iter_):\n",
    "    # Linear warmup for warmup_iters steps\n",
    "    if iter_ < warmup_iters:\n",
    "        return learning_rate * iter_ / warmup_iters\n",
    "    else:\n",
    "        # Cosine decay down to min learning rate\n",
    "        decay_ratio = (iter_ - warmup_iters) / (n_iters - warmup_iters)\n",
    "        assert 0 <= decay_ratio <= 1\n",
    "        coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) # coeff ranges 0..1\n",
    "        return min_learning_rate + coeff * (learning_rate - min_learning_rate)\n",
    "\n",
    "betas = (0.9, 0.95)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, betas=betas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T12:34:47.066951Z",
     "iopub.status.busy": "2023-09-10T12:34:47.066549Z",
     "iopub.status.idle": "2023-09-10T13:28:18.867993Z",
     "shell.execute_reply": "2023-09-10T13:28:18.866917Z",
     "shell.execute_reply.started": "2023-09-10T12:34:47.066919Z"
    },
    "id": "VlHr9hEtQFnN"
   },
   "outputs": [],
   "source": [
    "# TRAINING LOOP\n",
    "\n",
    "for iter_num in range(n_iters):\n",
    "    lr = get_lr(iter_num)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    # backprop and update the parameters\n",
    "    model.zero_grad(set_to_none=True)\n",
    "\n",
    "    for _ in range(grad_accum_steps):\n",
    "        x, y = fetch_sample(split=\"train\")\n",
    "        loss = model(x, y)\n",
    "        loss.backward()\n",
    "\n",
    "    optimizer.step()\n",
    "\n",
    "    if iter_num % 200 == 0:\n",
    "        train_loss = evaluate_loss(split=\"train\")\n",
    "        val_loss = evaluate_loss(\"val\")\n",
    "        print(f\"[{iter_num}/{n_iters}]: train={train_loss:.4f}, val={val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "KLS-BOpUQFnN"
   },
   "outputs": [],
   "source": [
    "val_loss = evaluate_loss(split=\"val\")\n",
    "print(f\"Final val loss: {val_loss:.4f}\")\n",
    "val_ppl = evaluate_val_perplexity()\n",
    "print(f\"Final val perplexity: {val_ppl:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T12:03:07.253374Z",
     "iopub.status.busy": "2023-09-10T12:03:07.253016Z",
     "iopub.status.idle": "2023-09-10T12:03:07.258831Z",
     "shell.execute_reply": "2023-09-10T12:03:07.257351Z",
     "shell.execute_reply.started": "2023-09-10T12:03:07.253344Z"
    },
    "id": "2W_4a5_rQFnN"
   },
   "outputs": [],
   "source": [
    "pf = lambda x: format_instruction({\"instruction\": x, \"input\": \"\", \"output\": \"\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-10T13:45:43.499598Z",
     "iopub.status.busy": "2023-09-10T13:45:43.498488Z",
     "iopub.status.idle": "2023-09-10T13:45:46.470772Z",
     "shell.execute_reply": "2023-09-10T13:45:46.469790Z",
     "shell.execute_reply.started": "2023-09-10T13:45:43.499538Z"
    },
    "id": "nKKJUjWRQFnN"
   },
   "outputs": [],
   "source": [
    "prompt = pf(\"Write a poem about deep learning.\")\n",
    "model.sample(prompt, tokenizer, temp=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LlqzedePYxhC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
